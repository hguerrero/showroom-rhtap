== AI with LLMs Chatbot Flow

AI/ML and specifically Large Language Models (LLM) for generative AI (GenAI) has been the hottest topic in the application development world since 2023 and the birth of OpenAI's ChatGPT on November 30 2022.

The https://en.wikipedia.org/wiki/Generative_pre-trained_transformer[GPT] and specifically the https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)[transformer architecture] has produced models that are incrediby capable at natural language processing (NLP) tasks making them ideally suited for conversational interactive user experience and the Hello World for an LLM is a chatbot.

*Narrator*:  I want to show you a template that provisions both the SDLC (Software Development Lifecycle) and the MDLC (Model Development Lifecycle). Where the SDLC is implemented as a Tekton-based Trusted Application Pipeline as seen in previous modules and the MDLC is implemented as a Red Hat OpenShift AI (RHOAI) pipeline based on Kubeflow.   

With open LLMs such as Llama, Mistral or Granite you can start with a foundation model that does not have to be trained from scratch. These models can be lifecycled, managed, served and monitored by Red Hat OpenShift AI while your application lifecycle remains independent and potentially hosted on OpenShift as will be seen in this demonstration.  And perhaps most importantly these open LLMs can live in your datacenter or VPC where your private data remains yours and does not have to traverse the internet.  

And in this case, we will be leveraging Red Hat Developer Hub, our supported and enterprise ready version of Backstage along with Red Hat Trusted Application Pipeline that leverages Trusted Arifact Signer, Trusted Profile Analyzer, Quay and Advanced Cluster Security. 

Let's see it in action, hopefully things become more clear as we go.

Click on *Create...*

image::chatbot-13a.png[]

Click *Choose* on the *Secured Chatbot with a Self-Hosted Large Language Model (LLM)* template

image::chatbot-14.png[]

Name: *employeebot*

Group Id: *redhat.janus*

Artifact Id: *employeebot*

Java Package Name: *org.redhat.janus*

Description: *A LLM infused employeebot app*

Click *Next*

Mode Name: *parasol-instruct*

image::chatbot-15.png[]

Narrator: These models have been curated and approved by Parasol's data science team. Parasol Insurance is a fictional company specializing in providing comprehensive insurance solutions.

Which models are listed in the template require a negotiation and collaboration with the folks who own models within Parasol, within your organization.  These models are under the governance of Red Hat OpenShift AI (RHOAI).

Click *Next*

For *Image Registry*, take all the defaults. 

image::chatbot-16.png[]

Click *Next*

For *Repository Location*, take all the defaults

image::chatbot-17.png[]

Click *Review*

image::chatbot-18.png[]

Click *Create*

The animation takes few seconds however this hides the heavy lifting happening under the covers.

Click on *Open Component in catalog*

image::chatbot-19.png[]

Narrator: The template wizard makes everything look super simple. Behind the scences is ArgoCD, OpenShift GitOps, that is actually doing the heavy lifting and provisioning everything.  Making this a self-service process for the user.  No tickets, no waiting.

Let's go check out the gitops repository 

Click *View Source*

image::chatbot-20.png[]

Click *development*

image::chatbot-21.png[]

Click *employeebot-gitops*

image::chatbot-22.png[]

Click *helm/ai/templates*

image::chatbot-23.png[]

Narrator: We could spend the next hour talking about the power of gitops but for now I wish to get back to the developer experience.  

Back to *Overview* tab

image::chatbot-24.png[]

Click *RHOAI Data Science Project*

image::chatbot-25.png[]

Narrator: The parasol-instruct model is based on Mistral-7B it was previously downloaded from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3[Huggingface], placed in cluster hoste storage, specifically an open source S3 solution called Minio.  This model is still very large and takes several minutes for the model server, the pod, to spin up and become ready.  We will look into the model serving and pipeline capabilities later in our demonstration.

For the sake of time, like in a TV cooking show, we will skip over to a project where the model server is already up and running.

Click on *Catalog* and *marketingbot* (or whatever name you gave it during the setup phase)

Click on *CI*

image::chatbot-26.png[]

Narrator: By default, the way this template is set up, a CI pipeline does not execute until AFTER the first *git commit* and *git push*.

Let's go make code change!

Click on *Overview* and *OpenShift Dev Spaces (VS Code)*

image::chatbot-27.png[]

image::chatbot-28.png[]

image::chatbot-29.png[]

image::chatbot-30.png[]

image::chatbot-31.png[]

image::chatbot-32.png[]

image::chatbot-33.png[]

image::chatbot-34.png[]

*Open a Terminal*

image::chatbot-35.png[]

----
mvn quarkus:dev
----

image::chatbot-36.png[]

image::chatbot-37.png[]

----
Do you agree to contribute anonymous build time data to the Quarkus community? (y/n and enter) 
----

*y* enter


image::chatbot-38.png[]

Click *Open in Preview*

image::chatbot-39.png[]

image::chatbot-40.png[]

----
why is the sky blue?
----

image::chatbot-41.png[]

=== Code Change

*src/main/java/com/redhat/Bot.java*

image::chatbot-42.png[]

The SystemMessage is where your provide the LLM with some upfront instructions, you can personify the AI. 

Some other SystemMessages that can be fun to demonstration include:

*Dracula*

----
   @SystemMessage("""
        You are an AI answering questions.

        Your response should be in the form of a Bram Stoker's Dracula

        When you don't know, respond with "We learn from failure, not from success!"

        Introduce yourself with: "I'm Dracula"
        """)
----

*Chuck Norris*

----
   @SystemMessage("""
        You are an AI answering questions.

        Your response should be in the form of a Chuck Norris joke

        When you don't know, respond with "Chuck Norris doesn't read books. He stares them down until he gets the information he wants."

        Introduce yourself with: "I'm Chuck Norris"
        """)
----

*Monty Python*

----
    @SystemMessage("""
        You are an AI answering questions.

        Your response should be as Monty Python's Black Knight

        When you don't know, respond with "None shall pass. I move for no man."

        Introduce yourself with: "I'm the Black Knight"
        """)
----

Replace the current SystemMessage

Click on *Simple Browser* and Refresh

image::chatbot-43.png[]

----
I have no quarrel with you, good Sir Knight, but I must cross this bridge.
----

image::chatbot-44.png[]


Open *src/resources/META-INF/resources/components/demo-title.js*

Search for *buddy* via Cntrl-F

Replace with the appropriate name like "Dracula", "Chuck" or "Black Knight"

image::chatbot-45.png[]

Click on *Simple Browser* and Refresh

image::chatbot-46.png[]

=== commit, push


Click the *Source Control* icon

Enter an appropriate commit message and click *Commit*

image::chatbot-47.png[]

Click *Always*

image::chatbot-48.png[]

Click *Sync Changes*

image::chatbot-49.png[]

Click *OK, Don't Show Again*

image::chatbot-50.png[]

Click *Yes* for periodically run "git fetch"?

image::chatbot-51.png[]

Click back to RHDH and the *CI* tab to see the Trusted Application Pipeline running

image::chatbot-52.png[]

This process takes between two minutes to four minutes.  Details about this pipeline are included in module *Pipeline Exploration*

image::chatbot-53.png[]

Once it is complete, click on the *Topology* tab and the URL for the application running in -dev

image::chatbot-54.png[]

image::chatbot-55.png[]









